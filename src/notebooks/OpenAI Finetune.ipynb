{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "openai.api_key = open(\"openai_api.key\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc(sent):\n",
    "    if not sent.endswith(\".\") or sent.endswith(\"!\"):  # finish with period\n",
    "        sent += '.'\n",
    "    if not sent[0].isupper():  # start with a capital letter\n",
    "        sent = sent[0].upper() + sent[1:]\n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train_xl.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_csv = [['prompt', 'completion']]\n",
    "for i, line in train_df.iterrows():\n",
    "    prompt = proc(line['startphrase']) + ' -> '\n",
    "    completion = proc(line[f'ending{line[\"labels\"]+1}'])\n",
    "    intermediate_csv.append([prompt, completion])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(intermediate_csv).to_csv(\"data/finetune_train_xl.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing...\n",
      "\n",
      "- Based on your file extension, your file is formatted as a CSV file\n",
      "- Your file contains 8016 prompt-completion pairs\n",
      "- There are 4 duplicated prompt-completion pairs. These are rows: [1749, 3965, 4061, 7394]\n",
      "- All prompts end with suffix `. -> `\n",
      "- All completions end with suffix `.`\n",
      "  WARNING: Some of your completions contain the suffix `.` more than once. We suggest that you review your completions and add a unique ending\n",
      "- The completion should start with a whitespace character (` `). This tends to produce better results due to the tokenization we use. See https://beta.openai.com/docs/guides/fine-tuning/preparing-your-dataset for more details\n",
      "\n",
      "Based on the analysis we will perform the following actions:\n",
      "- [Necessary] Your format `CSV` will be converted to `JSONL`\n",
      "- [Recommended] Remove 4 duplicate rows [Y/n]: ^C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!openai tools fine_tunes.prepare_data -f data/finetune_train_xl.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!openai api fine_tunes.create -t data/finetune_train_xl_prepared.jsonl  -m ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADA_FINETUNED = 'ada:ft-user-6qia53bwp385gfq1da9w5yum-2021-11-28-03-10-25'\n",
    "BABBAGE_FINETUNED = 'babbage:ft-user-6qia53bwp385gfq1da9w5yum-2021-11-28-04-06-02'\n",
    "CURIE_FINETUNED = 'curie:ft-user-6qia53bwp385gfq1da9w5yum-2021-11-28-04-35-14'\n",
    "ADA_FINETUNED_XL = \"ada:ft-user-6qia53bwp385gfq1da9w5yum-2021-12-11-18-29-00\"\n",
    "BABBAGE_FINETUNED_XL = \"babbage:ft-user-6qia53bwp385gfq1da9w5yum-2021-12-11-18-53-57\"\n",
    "CURIE_FINETUNED_XL = \"curie:ft-user-6qia53bwp385gfq1da9w5yum-2021-12-11-18-36-40\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split = 'test'\n",
    "df = pd.read_csv(f\"data/{split}.csv\")\n",
    "json_lines = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "restart = None\n",
    "\n",
    "def gpt3_finetune(model_name, suffix_prompt='', use_proc_lower=False):\n",
    "    assert suffix_prompt == '' or suffix_prompt.startswith(' ')\n",
    "    proc2 = proc_lower if use_proc_lower else proc\n",
    "    global json_lines\n",
    "    if restart is None:\n",
    "        json_lines = {}\n",
    "\n",
    "    if model_name != 'debug':\n",
    "        response = input(f\"about the spend $$$ on openai API (model {model_name})! conitnue? [y/n]\")\n",
    "        if response.lower() != 'y':\n",
    "            raise Exception(\"Not continuing.\")\n",
    "    else:\n",
    "        print('just debugging. this is free.')\n",
    "\n",
    "\n",
    "    for i, line in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        if restart is not None and i < restart: continue\n",
    "\n",
    "        start = line['startphrase']\n",
    "        end1 = line['ending1']\n",
    "        end2 = line['ending2']\n",
    "        res_two_endings = []\n",
    "        for j, end in enumerate((end1, end2)):\n",
    "            if model_name == 'debug':\n",
    "                res = debug_res\n",
    "            else:\n",
    "                prompt = proc(start) + suffix_prompt + ' ' + proc2(end)\n",
    "                if i < 3:\n",
    "                    print(\"prompt is:\", prompt)\n",
    "                completion = openai.Completion.create(model=model_name, prompt=prompt,\n",
    "                                                          max_tokens=0,\n",
    "                                                          temperature=0.0,\n",
    "                                                          logprobs=0,\n",
    "                                                          echo=True,\n",
    "                                                          n=1)\n",
    "                logprobs = completion['choices'][0]['logprobs']\n",
    "                res = {k: logprobs[k] for k in ('token_logprobs', 'tokens')}\n",
    "            res_two_endings.append(res)\n",
    "            if model_name != 'debug':\n",
    "                time.sleep(0.05)  # to prevent RateLimitError\n",
    "        json_lines[f\"{line.get('qid', i)}_{line['labels']}\"] = res_two_endings\n",
    "\n",
    "\n",
    "    fname = f\"{split}_logprobs_{model_name}{suffix_prompt}.json\"\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write('')\n",
    "\n",
    "    with open(fname, 'a') as f:\n",
    "        json.dump(json_lines, f, indent=2)\n",
    "    print(\"DONE. Dumped:\", fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "restart = None\n",
    "\n",
    "def gpt3_finetune_batch(model_name, suffix_prompt='', use_proc_lower=False):\n",
    "    assert suffix_prompt == '' or suffix_prompt.startswith(' ')\n",
    "    proc2 = proc_lower if use_proc_lower else proc\n",
    "    global json_lines\n",
    "    if restart is None:\n",
    "        json_lines = {}\n",
    "\n",
    "    if model_name != 'debug':\n",
    "        response = input(f\"about the spend $$$ on openai API (model {model_name})! conitnue? [y/n]\")\n",
    "        if response.lower() != 'y':\n",
    "            raise Exception(\"Not continuing.\")\n",
    "    else:\n",
    "        print('just debugging. this is free.')\n",
    "\n",
    "    prompts = [[], []]  # first completion, second completion\n",
    "    for i, line in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        if restart is not None and i < restart: continue\n",
    "\n",
    "        start = line['startphrase']\n",
    "        end1 = line['ending1']\n",
    "        end2 = line['ending2']\n",
    "        for j, end in enumerate((end1, end2)):\n",
    "            prompt = proc(start) + suffix_prompt + ' ' + proc2(end)\n",
    "            prompts[j].append(prompt)\n",
    "            if i < 3:\n",
    "                print(\"prompt is:\", prompt)\n",
    "                    \n",
    "    print(\"Calling API...\")\n",
    "    t0 = time.time()\n",
    "    completions = []\n",
    "    for j in (0, 1):\n",
    "        completion = openai.Completion.create(model=model_name, prompt=prompts[j],\n",
    "                                                  max_tokens=0,\n",
    "                                                  temperature=0.0,\n",
    "                                                  logprobs=0,\n",
    "                                                  echo=True,\n",
    "                                                  n=1)\n",
    "        completions.append(completion)\n",
    "    t1 = time.time()\n",
    "    print(f\"API results received, took {t1-t0} seconds\")\n",
    "    for i, line in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        res_two_endings = []\n",
    "        for j in (0, 1):\n",
    "            try:\n",
    "                logprobs = completions[j]['choices'][i]['logprobs']\n",
    "                res = {k: logprobs[k] for k in ('token_logprobs', 'tokens')}\n",
    "                res_two_endings.append(res)\n",
    "            except IndexError:\n",
    "                print(f'cannot fine i={i} j={j}')\n",
    "        json_lines[f\"{line.get('qid', i)}_{line['labels']}\"] = res_two_endings\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    fname = f\"{split}_logprobs_{model_name}{suffix_prompt}.json\"\n",
    "    with open(fname, 'w') as f:\n",
    "        f.write('')\n",
    "\n",
    "    with open(fname, 'a') as f:\n",
    "        json.dump(json_lines, f, indent=2)\n",
    "    print(\"DONE. Dumped:\", fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about the spend $$$ on openai API (model babbage:ft-user-6qia53bwp385gfq1da9w5yum-2021-12-11-18-53-57)! conitnue? [y/n]y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1146/1146 [00:00<00:00, 17189.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt is: The girl was as down-to-earth as a Michelin-starred canape. The girl was not down-to-earth at all.\n",
      "prompt is: The girl was as down-to-earth as a Michelin-starred canape. The girl was very down-to-earth.\n",
      "prompt is: The girl was as down-to-earth as eggs and potatoes. The girl was not down-to-earth at all.\n",
      "prompt is: The girl was as down-to-earth as eggs and potatoes. The girl was very down-to-earth.\n",
      "prompt is: The girl's room was as messy as pig slops. The girl's room was a total mess.\n",
      "prompt is: The girl's room was as messy as pig slops. The girl's room was very clean.\n",
      "Calling API...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1146/1146 [00:00<00:00, 20903.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API results received, took 9.419008731842041 seconds\n",
      "DONE. Dumped: test_logprobs_babbage:ft-user-6qia53bwp385gfq1da9w5yum-2021-12-11-18-53-57.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gpt3_finetune_batch(BABBAGE_FINETUNED_XL, suffix_prompt='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_of_ending(token_logprobs, tokens):\n",
    "    logprob_sum = 0\n",
    "    for count, (lp, t) in enumerate(zip(token_logprobs[::-1], tokens[::-1])):\n",
    "        if count > 0 and t.endswith('.'):\n",
    "            break\n",
    "        logprob_sum += lp\n",
    "    return logprob_sum / count\n",
    "\n",
    "\n",
    "def calculate_accuracy(fname):\n",
    "    with open(fname) as f:\n",
    "        logprobs = json.load(f)\n",
    "\n",
    "    correct = 0\n",
    "    for qid_label, (end1, end2) in logprobs.items():\n",
    "        end1_prob = prob_of_ending(end1['token_logprobs'], end1['tokens'])\n",
    "        end2_prob = prob_of_ending(end2['token_logprobs'], end2['tokens'])\n",
    "        label = int(qid_label[-1])\n",
    "        if (label == 0 and end1_prob > end2_prob) or (label==1 and end1_prob < end2_prob):\n",
    "            correct += 1\n",
    "\n",
    "    print(f\"correct: {correct}/{len(logprobs)} = {correct/len(logprobs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 794/1094 = 0.7257769652650823\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(f\"dev_logprobs_{ADA_FINETUNED}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 832/1094 = 0.7605118829981719\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(f\"dev_logprobs_{BABBAGE_FINETUNED}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 866/1094 = 0.7915904936014625\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(f\"dev_logprobs_{CURIE_FINETUNED}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 792/1145 = 0.6917030567685589\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(f\"test_logprobs_{ADA_FINETUNED}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 843/1146 = 0.7356020942408377\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(f\"test_logprobs_{ADA_FINETUNED_XL}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 790/1146 = 0.6893542757417103\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(f\"test_logprobs_{ADA_FINETUNED} -> .json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 841/1146 = 0.7338568935427574\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(f\"test_logprobs_{ADA_FINETUNED_XL} -> .json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 847/1145 = 0.7397379912663755\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(f\"test_logprobs_{BABBAGE_FINETUNED}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 886/1146 = 0.7731239092495636\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(f\"test_logprobs_{BABBAGE_FINETUNED_XL}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 905/1145 = 0.7903930131004366\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(f\"test_logprobs_{CURIE_FINETUNED}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 910/1146 = 0.794066317626527\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(f\"test_logprobs_{CURIE_FINETUNED} -> .json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 916/1146 = 0.7993019197207679\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(f\"test_logprobs_{CURIE_FINETUNED_XL}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 939/1146 = 0.819371727748691\n"
     ]
    }
   ],
   "source": [
    "calculate_accuracy(f\"test_logprobs_{CURIE_FINETUNED_XL} -> .json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What kind of sentences does finetuning allow GPT3 to get it right? Are there sentences that GPT got wrong after finetuning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_og = pd.read_csv(\"gpt3_probabilities_curie_test.csv\")\n",
    "df_ft = pd.read_csv(\"gpt3_probabilities_finetuned_curie_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_csv_rows = [['startphrase', 'ending1', 'ending2', 'correct_label', 'orig_correct', 'finetune_correct']]\n",
    "for n, ft_row in df_ft.iterrows():\n",
    "    og_row = df_og[df_og.x_1 == ft_row.x_1].squeeze()\n",
    "    for i in (1, 2):\n",
    "        og_correct = int(og_row[f'P(y_{i}|x_{i})'] > 0.5)\n",
    "        ft_correct = int(ft_row[f'P(y_{i}|x_{i})'] > 0.5)\n",
    "        out_csv_rows.append([og_row[f'x_{i}'], og_row[f'y_1'], og_row[f'y_2'], (i-1), og_correct, ft_correct])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(out_csv_rows).to_csv(\"curie_finetuning_comparison_test.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x_1            George ran like a bat out of hell\n",
       "x_2                      George ran like a snail\n",
       "y_1                             George ran fast.\n",
       "y_2                             George ran slow.\n",
       "P(x_1, y_1)                             0.039823\n",
       "P(x_1, y_2)                             0.025864\n",
       "P(x_2, y_1)                             0.010111\n",
       "P(x_2, y_2)                             0.008083\n",
       "P(y_1|x_1)                              0.606258\n",
       "P(y_2|x_2)                              0.444266\n",
       "P(x_1|y_1)                              0.797508\n",
       "P(x_2|y_2)                              0.238114\n",
       "Name: 256, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run gpt with an array of prompts???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ada'\n",
    "prompts = [\n",
    "    \"The girl was as down-to-earth as a Michelin-starred canape. ->  The girl was not down-to-earth at all.\",\n",
    "    \"The girl was as down-to-earth as a Michelin-starred canape. ->  The girl was very down-to-earth.\"\n",
    "]\n",
    "completion = openai.Completion.create(model=model_name, prompt=prompts,\n",
    "                                                          max_tokens=0,\n",
    "                                                          temperature=0.0,\n",
    "                                                          logprobs=0,\n",
    "                                                          echo=True,\n",
    "                                                          n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-e69884607197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompletion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'choices'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py38] *",
   "language": "python",
   "name": "conda-env-py38-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
